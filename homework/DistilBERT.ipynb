{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('labeled_data_corpus.csv')\n",
    "train_df = raw_df.loc[raw_df['subset'] == 'train']\n",
    "test_df = raw_df.loc[raw_df['subset'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, frame: pd.DataFrame, tokenizer):\n",
    "      self.inputs = []\n",
    "      self.labels = []\n",
    "      for index, row in frame.iterrows():\n",
    "          inputs = tokenizer(row['msg'], return_tensors=\"pt\", max_length=200, truncation=True, padding='max_length')\n",
    "          input_ids = inputs['input_ids'][0]\n",
    "          self.inputs.append(input_ids)\n",
    "          self.labels.append(row['label'])\n",
    "  def __len__(self):\n",
    "      return len(self.labels)\n",
    "  def __getitem__(self, index):\n",
    "      return {\"input_ids\" : self.inputs[index], 'labels': torch.tensor(self.labels[index])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795b644a6653444aa9fe4a02e96f653c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce4aae643ec4ceb8938380065c9bb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6d084ca97b43e98a4a45a251d0555f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_dataset = TextDataset(train_df, tokenizer)\n",
    "test_dataset = TextDataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "  def __init__(self, criterion):\n",
    "    super().__init__()\n",
    "    self.bert = transformers.DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "    self.criterion = criterion\n",
    "\n",
    "  def forward(self, input_ids, labels):\n",
    "    out = self.bert(input_ids).logits\n",
    "    if labels is not None:\n",
    "      loss = self.criterion(out, labels)\n",
    "      return loss, out.max(dim=1).indices\n",
    "    else:\n",
    "      return (out.max(dim=1).indices, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics (evalPrediction):\n",
    "    predictions = evalPrediction.predictions\n",
    "    label_ids = evalPrediction.label_ids\n",
    "    print(predictions)\n",
    "    print(label_ids)\n",
    "    return {\n",
    "        \"f1\": f1_score(label_ids, predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0febaaa0c02f439bbb67e58ae858333c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5233\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20c446af6c84cc8887016c6d66533c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1309\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3e07df0d364d1c8626e97ec5c1910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[1 0 0 ... 0 0 0]\n",
      "{'eval_loss': 0.501939058303833, 'eval_f1': 0.0, 'eval_runtime': 222.7818, 'eval_samples_per_second': 5.876, 'eval_steps_per_second': 0.368, 'epoch': 1.0}\n",
      "{'train_runtime': 3086.88, 'train_samples_per_second': 1.695, 'train_steps_per_second': 0.106, 'train_loss': 0.5198788991788539, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=328, training_loss=0.5198788991788539, metrics={'train_runtime': 3086.88, 'train_samples_per_second': 1.695, 'train_steps_per_second': 0.106, 'train_loss': 0.5198788991788539, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = transformers.TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size  = 16,\n",
    "    learning_rate = 0.001,\n",
    "    num_train_epochs=1,\n",
    "    fp16 = False\n",
    "\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = Model(criterion)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args =args ,\n",
    "    train_dataset =train_dataset,\n",
    "    eval_dataset  = test_dataset ,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cff3abf1678755e0069fd79299a535fe1940bcd71a6b01d9f4386710b2b163f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
